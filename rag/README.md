---
language:
- en
- ko
license: llama3
library_name: transformers
tags:
- llama-cpp
- gguf-my-repo
base_model:
- meta-llama/Meta-Llama-3-8B
- jeiku/Average_Test_v1
- MLP-KTLim/llama-3-Korean-Bllossom-8B
---


<a href="https://github.com/MLP-Lab/Bllossom">
  <img src="https://github.com/teddysum/bllossom/blob/main//bllossom_icon.png?raw=true" width="40%" height="50%">
</a>



# Update!
* [2024.06.18] ì‚¬ì „í•™ìŠµëŸ‰ì„ **250GB**ê¹Œì§€ ëŠ˜ë¦° Bllossom ELOëª¨ë¸ë¡œ ì—…ë°ì´íŠ¸ ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ë‹¨ì–´í™•ì¥ì€ í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ì¡´ ë‹¨ì–´í™•ì¥ëœ long-context ëª¨ë¸ì„ í™œìš©í•˜ê³  ì‹¶ìœ¼ì‹ ë¶„ì€ ê°œì¸ì—°ë½ì£¼ì„¸ìš”!
* [2024.06.18] Bllossom ELO ëª¨ë¸ì€ ìì²´ ê°œë°œí•œ ELOì‚¬ì „í•™ìŠµ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ í•™ìŠµëœ ëª¨ë¸ì…ë‹ˆë‹¤. [LogicKor](https://github.com/StableFluffy/LogicKor) ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ í˜„ì¡´í•˜ëŠ” í•œêµ­ì–´ 10Bì´í•˜ ëª¨ë¸ì¤‘ SOTAì ìˆ˜ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤. 

LogicKor ì„±ëŠ¥í‘œ :
| Model | Math | Reasoning | Writing | Coding | Understanding | Grammar | Single ALL | Multi ALL | Overall |
|:---------:|:-----:|:------:|:-----:|:-----:|:----:|:-----:|:-----:|:-----:|:----:|
| gpt-3.5-turbo-0125 | 7.14 | 7.71 | 8.28 | 5.85 | 9.71 | 6.28 | 7.50 | 7.95 | 7.72 |
| gemini-1.5-pro-preview-0215 | 8.00 | 7.85 | 8.14 | 7.71 | 8.42 | 7.28 | 7.90 | 6.26 | 7.08 |
| llama-3-Korean-Bllossom-8B | 5.43 | 8.29 | 9.0 | 4.43 | 7.57 | 6.86 | 6.93 | 6.93 | 6.93 |


# Bllossom | [Demo]() | [Homepage](https://www.bllossom.ai/) | [Github](https://github.com/MLP-Lab/Bllossom)

- ë³¸ ëª¨ë¸ì€ CPUì—ì„œ êµ¬ë™ê°€ëŠ¥í•˜ë©° ë¹ ë¥¸ ì†ë„ë¥¼ ìœ„í•´ì„œëŠ” 8GB GPUì—ì„œ êµ¬ë™ ê°€ëŠ¥í•œ ì–‘ìí™” ëª¨ë¸ì…ë‹ˆë‹¤! [Colab ì˜ˆì œ](https://colab.research.google.com/drive/129ZNVg5R2NPghUEFHKF0BRdxsZxinQcJ?usp=drive_link) |

```bash
ì €í¬ BllossomíŒ€ ì—ì„œ í•œêµ­ì–´-ì˜ì–´ ì´ì¤‘ ì–¸ì–´ëª¨ë¸ì¸ Bllossomì„ ê³µê°œí–ˆìŠµë‹ˆë‹¤!
ì„œìš¸ê³¼ê¸°ëŒ€ ìŠˆí¼ì»´í“¨íŒ… ì„¼í„°ì˜ ì§€ì›ìœ¼ë¡œ 100GBê°€ë„˜ëŠ” í•œêµ­ì–´ë¡œ ëª¨ë¸ì „ì²´ë¥¼ í’€íŠœë‹í•œ í•œêµ­ì–´ ê°•í™” ì´ì¤‘ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤!
í•œêµ­ì–´ ì˜í•˜ëŠ” ëª¨ë¸ ì°¾ê³  ìˆì§€ ì•Šìœ¼ì…¨ë‚˜ìš”?
 - í•œêµ­ì–´ ìµœì´ˆ! ë¬´ë ¤ 3ë§Œê°œê°€ ë„˜ëŠ” í•œêµ­ì–´ ì–´íœ˜í™•ì¥
 - Llama3ëŒ€ë¹„ ëŒ€ëµ 25% ë” ê¸´ ê¸¸ì´ì˜ í•œêµ­ì–´ Context ì²˜ë¦¬ê°€ëŠ¥
 - í•œêµ­ì–´-ì˜ì–´ Pararell Corpusë¥¼ í™œìš©í•œ í•œêµ­ì–´-ì˜ì–´ ì§€ì‹ì—°ê²° (ì‚¬ì „í•™ìŠµ)
 - í•œêµ­ì–´ ë¬¸í™”, ì–¸ì–´ë¥¼ ê³ ë ¤í•´ ì–¸ì–´í•™ìê°€ ì œì‘í•œ ë°ì´í„°ë¥¼ í™œìš©í•œ ë¯¸ì„¸ì¡°ì •
 - ê°•í™”í•™ìŠµ
ì´ ëª¨ë“ ê²Œ í•œêº¼ë²ˆì— ì ìš©ë˜ê³  ìƒì—…ì  ì´ìš©ì´ ê°€ëŠ¥í•œ Bllossomì„ ì´ìš©í•´ ì—¬ëŸ¬ë¶„ ë§Œì˜ ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ì„¸ìš¥!
ë³¸ ëª¨ë¸ì€ CPUì—ì„œ êµ¬ë™ê°€ëŠ¥í•˜ë©° ë¹ ë¥¸ ì†ë„ë¥¼ ìœ„í•´ì„œëŠ” 6GB GPUì—ì„œ êµ¬ë™ ê°€ëŠ¥í•œ ì–‘ìí™” ëª¨ë¸ì…ë‹ˆë‹¤!

1. Bllossom-8BëŠ” ì„œìš¸ê³¼ê¸°ëŒ€, í…Œë””ì¸, ì—°ì„¸ëŒ€ ì–¸ì–´ìì› ì—°êµ¬ì‹¤ì˜ ì–¸ì–´í•™ìì™€ í˜‘ì—…í•´ ë§Œë“  ì‹¤ìš©ì£¼ì˜ê¸°ë°˜ ì–¸ì–´ëª¨ë¸ì…ë‹ˆë‹¤! ì•ìœ¼ë¡œ ì§€ì†ì ì¸ ì—…ë°ì´íŠ¸ë¥¼ í†µí•´ ê´€ë¦¬í•˜ê² ìŠµë‹ˆë‹¤ ë§ì´ í™œìš©í•´ì£¼ì„¸ìš” ğŸ™‚
2. ì´ˆ ê°•ë ¥í•œ Advanced-Bllossom 8B, 70Bëª¨ë¸, ì‹œê°-ì–¸ì–´ëª¨ë¸ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤! (ê¶ê¸ˆí•˜ì‹ ë¶„ì€ ê°œë³„ ì—°ë½ì£¼ì„¸ìš”!!)
3. Bllossomì€ NAACL2024, LREC-COLING2024 (êµ¬ë‘) ë°œí‘œë¡œ ì±„íƒë˜ì—ˆìŠµë‹ˆë‹¤.
4. ì¢‹ì€ ì–¸ì–´ëª¨ë¸ ê³„ì† ì—…ë°ì´íŠ¸ í•˜ê² ìŠµë‹ˆë‹¤!! í•œêµ­ì–´ ê°•í™”ë¥¼ìœ„í•´ ê³µë™ ì—°êµ¬í•˜ì‹¤ë¶„(íŠ¹íˆë…¼ë¬¸) ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤!! 
   íŠ¹íˆ ì†ŒëŸ‰ì˜ GPUë¼ë„ ëŒ€ì—¬ ê°€ëŠ¥í•œíŒ€ì€ ì–¸ì œë“  ì—°ë½ì£¼ì„¸ìš”! ë§Œë“¤ê³  ì‹¶ì€ê±° ë„ì™€ë“œë ¤ìš”.
```

The Bllossom language model is a Korean-English bilingual language model based on the open-source LLama3. It enhances the connection of knowledge between Korean and English. It has the following features:

* **Knowledge Linking**: Linking Korean and English knowledge through additional training
* **Vocabulary Expansion**: Expansion of Korean vocabulary to enhance Korean expressiveness.
* **Instruction Tuning**: Tuning using custom-made instruction following data specialized for Korean language and Korean culture
* **Human Feedback**: DPO has been applied
* **Vision-Language Alignment**: Aligning the vision transformer with this language model 

**This model developed by [MLPLab at Seoultech](http://mlp.seoultech.ac.kr), [Teddysum](http://teddysum.ai/) and [Yonsei Univ](https://sites.google.com/view/hansaemkim/hansaem-kim).** 
**This model was converted to GGUF format from [`MLP-KTLim/llama-3-Korean-Bllossom-8B`](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B) using llama.cpp via the ggml.ai's [GGUF-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo) space.
Refer to the [original model card](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B) for more details on the model.**


## Demo Video

<div style="display: flex; justify-content: space-between;">
  <!-- ì²« ë²ˆì§¸ ì»¬ëŸ¼ -->
  <div style="width: 49%;">
    <a>
      <img src="https://github.com/lhsstn/lhsstn/blob/main/x-llava_dem.gif?raw=true" style="width: 100%; height: auto;">
    </a>
    <p style="text-align: center;">Bllossom-V Demo</p>
  </div>

  <!-- ë‘ ë²ˆì§¸ ì»¬ëŸ¼ (í•„ìš”í•˜ë‹¤ë©´) -->
  <div style="width: 49%;">
    <a>
      <img src="https://github.com/lhsstn/lhsstn/blob/main/bllossom_demo_kakao.gif?raw=true" style="width: 70%; height: auto;">
    </a>
    <p style="text-align: center;">Bllossom Demo(Kakao)ã…¤ã…¤ã…¤ã…¤ã…¤ã…¤ã…¤ã…¤</p>
  </div>
</div>



## NEWS
* [2024.05.08] Vocab Expansion Model Update
* [2024.04.25] We released Bllossom v2.0, based on llama-3
* [2023/12] We released Bllossom-Vision v1.0, based on Bllossom
* [2023/08] We released Bllossom v1.0, based on llama-2. 
* [2023/07] We released Bllossom v0.7, based on polyglot-ko.


## Example code
```python
!CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python
!huggingface-cli download MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M --local-dir='YOUR-LOCAL-FOLDER-PATH'

from llama_cpp import Llama
from transformers import AutoTokenizer

model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M'
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = Llama(
    model_path='YOUR-LOCAL-FOLDER-PATH/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf',
    n_ctx=512,
    n_gpu_layers=-1        # Number of model layers to offload to GPU
)

PROMPT = \
'''ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆì˜ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
You are a helpful AI assistant, you'll need to answer users' queries in a friendly and accurate manner.'''

instruction = 'Your Instruction'

messages = [
    {"role": "system", "content": f"{PROMPT}"},
    {"role": "user", "content": f"{instruction}"}
    ]

prompt = tokenizer.apply_chat_template(
    messages, 
    tokenize = False,
    add_generation_prompt=True
)

generation_kwargs = {
    "max_tokens":512,
    "stop":["<|eot_id|>"],
    "top_p":0.9,
    "temperature":0.6,
    "echo":True, # Echo the prompt in the output
}

resonse_msg = model(prompt, **generation_kwargs)
print(resonse_msg['choices'][0]['text'][len(prompt):])
```



## Citation
**Language Model**
```text
@misc{bllossom,
  author = {ChangSu Choi, Yongbin Jeong, Seoyoon Park, InHo Won, HyeonSeok Lim, SangMin Kim, Yejee Kang, Chanhyuk Yoon, Jaewan Park, Yiseul Lee, HyeJin Lee, Younggyun Hahm, Hansaem Kim, KyungTae Lim},
  title = {Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean},
  year = {2024},
  journal = {LREC-COLING 2024},
  paperLink = {\url{https://arxiv.org/pdf/2403.10882}},
 },
}
```

**Vision-Language Model**
```text
@misc{bllossom-V,
  author = {Dongjae Shin, Hyunseok Lim, Inho Won, Changsu Choi, Minjun Kim, Seungwoo Song, Hangyeol Yoo, Sangmin Kim, Kyungtae Lim},
  title = {X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment},
  year = {2024},
  publisher = {GitHub},
  journal = {NAACL 2024 findings},
  paperLink = {\url{https://arxiv.org/pdf/2403.11399}},
 },
}
```

## Contact
 - ì„ê²½íƒœ(KyungTae Lim), Professor at Seoultech. `ktlim@seoultech.ac.kr`
 - í•¨ì˜ê· (Younggyun Hahm), CEO of Teddysum. `hahmyg@teddysum.ai`
 - ê¹€í•œìƒ˜(Hansaem Kim), Professor at Yonsei. `khss@yonsei.ac.kr`

## Contributor
 - ìµœì°½ìˆ˜(Chansu Choi), choics2623@seoultech.ac.kr
 - ê¹€ìƒë¯¼(Sangmin Kim), sangmin9708@naver.com
 - ì›ì¸í˜¸(Inho Won), wih1226@seoultech.ac.kr
 - ê¹€ë¯¼ì¤€(Minjun Kim), mjkmain@seoultech.ac.kr 
 - ì†¡ìŠ¹ìš°(Seungwoo Song), sswoo@seoultech.ac.kr
 - ì‹ ë™ì¬(Dongjae Shin), dylan1998@seoultech.ac.kr
 - ì„í˜„ì„(Hyeonseok Lim), gustjrantk@seoultech.ac.kr
 - ìœ¡ì •í›ˆ(Jeonghun Yuk), usually670@gmail.com
 - ìœ í•œê²°(Hangyeol Yoo), 21102372@seoultech.ac.kr
 - ì†¡ì„œí˜„(Seohyun Song), alexalex225225@gmail.com